"""Utilities for importing extracted JSON files into PostgreSQL.

This module expects a PostgreSQL database with the schema defined in
``db/schema_postgres.sql``.  It can be used as a library or as a command
line tool to initialise the schema and import one or more directories of
JSON files generated by the processing pipeline.
"""

from __future__ import annotations
import argparse
import glob
import json
import os
from pathlib import Path
from typing import Iterable
import re

from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool
from highlight import canonical_num

try:  # optional dependency for directory watching
    from watchdog.events import FileSystemEventHandler  # type: ignore
    from watchdog.observers import Observer  # type: ignore
except Exception:  # pragma: no cover - watchdog not installed
    FileSystemEventHandler = object  # type: ignore
    Observer = None

PG_DSN = os.environ.get(
    "PG_DSN", "postgresql+psycopg:///legislation"
)
engine = create_engine(PG_DSN, poolclass=QueuePool, pool_size=10, max_overflow=20)


def init_db() -> None:
    """Create tables in the configured PostgreSQL database if needed."""
    schema_path = Path(__file__).with_name("schema_postgres.sql")
    ddl = schema_path.read_text(encoding="utf-8")
    with engine.begin() as conn:
        # ``schema_postgres.sql`` now ensures the ``global_id`` column and index
        # exist even for databases created with an older schema.
        conn.execute(text(ddl))


LAW_RE = re.compile(r"القانون\s+رقم\s*(\d+(?:[./]\d+)*)")


def _extract_law_number(meta: dict, file_name: str) -> str | None:
    candidates = [meta.get("law_number"), meta.get("official_title"), meta.get("short_title"), file_name]
    for cand in candidates:
        if not cand:
            continue
        m = LAW_RE.search(cand)
        if m:
            num = canonical_num(m.group(1))
            if num:
                return num
    for ref in meta.get("references", []):
        typ = ref.get("type") or ""
        if "قانون" in typ:
            num = canonical_num(ref.get("reference_number"))
            if num:
                return num
    return None


def upsert_document(conn, file_name, short_title, doc_number, law_number):
    r = conn.execute(
        text(
            """
        INSERT INTO documents(file_name, short_title, doc_number, law_number)
        VALUES (:f,:s,:n,:l)
        ON CONFLICT (file_name) DO UPDATE SET short_title=EXCLUDED.short_title, doc_number=EXCLUDED.doc_number, law_number=EXCLUDED.law_number
        RETURNING id
    """
        ),
        dict(f=file_name, s=short_title, n=doc_number, l=law_number),
    ).first()
    return r[0]

def _import_file(conn, p: str) -> None:
    data = json.load(open(p, "r", encoding="utf-8"))
    file_name = os.path.basename(p)
    if file_name.endswith("_ner.json") and "metadata" not in data:
        base_file = file_name.replace("_ner", "")
        row = conn.execute(
            text("SELECT id FROM documents WHERE file_name=:f"),
            {"f": base_file},
        ).first()
        if not row:
            return
        doc_id = row[0]
    else:
        meta = data.get("metadata", {})
        short = (
            meta.get("short_title")
            or meta.get("official_title")
            or file_name
        )
        docn = canonical_num(meta.get("document_number") or meta.get("number"))
        law_num = _extract_law_number(meta, file_name)
        doc_id = upsert_document(conn, file_name, short, docn, law_num)
        conn.execute(
            text("DELETE FROM articles WHERE document_id=:d"),
            {"d": doc_id},
        )

        def _collect(nodes: list[dict]) -> None:
            for node in nodes:
                typ = node.get("type")
                if typ in {"ARTICLE", "الفصل", "مادة"}:
                    conn.execute(
                        text(
                            """
                INSERT INTO articles(document_id, number, text)
                VALUES (:d, :n, :t)
            """
                        ),
                        {
                            "d": doc_id,
                            "n": node.get("number")
                            or node.get("normalized")
                            or node.get("title"),
                            "t": node.get("text") or "",
                        },
                    )
                children = node.get("children")
                if isinstance(children, list):
                    _collect(children)

        _collect(data.get("structure", []))

    ner_root = data.get("ner_result") or data
    ents = ner_root.get("entities", [])
    conn.execute(
        text("DELETE FROM entities WHERE document_id=:d"),
        {"d": doc_id},
    )
    for e in ents:
        conn.execute(
            text(
                """
            INSERT INTO entities(document_id, type, text, normalized, global_id)
            VALUES (:d, :ty, :tx, :nz, :gid)
        """
            ),
            {
                "d": doc_id,
                "ty": e.get("type"),
                "tx": e.get("text"),
                "nz": e.get("normalized") or e.get("text"),
                "gid": e.get("global_id"),
            },
        )

    rels = ner_root.get("relations", [])
    conn.execute(
        text("DELETE FROM relations WHERE document_id=:d"),
        {"d": doc_id},
    )
    for r in rels:
        conn.execute(
            text(
                """
            INSERT INTO relations(document_id, source_id, target_id, type)
            VALUES (:d, :s, :t, :ty)
        """
            ),
            {
                "d": doc_id,
                "s": r.get("source_id"),
                "t": r.get("target_id"),
                "ty": r.get("type"),
            },
        )


def import_json_file(path: str) -> None:
    """Import a single JSON *path* into the database."""
    init_db()
    with engine.begin() as conn:
        _import_file(conn, path)


def import_json_dir(path: str) -> None:
    """Import all ``*.json`` files found directly under *path*."""
    init_db()
    with engine.begin() as conn:
        for p in glob.glob(os.path.join(path, "*.json")):
            _import_file(conn, p)


def import_paths(paths: Iterable[str]) -> None:
    """Import JSON files from each directory in *paths*."""
    init_db()
    for p in paths:
        import_json_dir(p)


_observer: Observer | None = None


def watch_dirs(paths: Iterable[str]) -> None:
    """Watch *paths* for new or modified JSON files and import them automatically."""
    global _observer
    if Observer is None or _observer is not None:
        return

    class Handler(FileSystemEventHandler):
        def _handle(self, event):
            if getattr(event, "is_directory", False):
                return
            if event.src_path.endswith(".json"):
                try:
                    import_json_file(event.src_path)
                except Exception:
                    pass

        def on_created(self, event):  # pragma: no cover - file system events
            self._handle(event)

        def on_modified(self, event):  # pragma: no cover - file system events
            self._handle(event)

    observer = Observer()
    handler = Handler()
    scheduled = False
    for p in paths:
        if os.path.isdir(p):
            observer.schedule(handler, p, recursive=False)
            scheduled = True
    if scheduled:
        observer.daemon = True
        observer.start()
        _observer = observer


def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(
        description="Import extracted JSON data into PostgreSQL"
    )
    parser.add_argument(
        "paths",
        nargs="*",
        default=["output", "legal_output"],
        help="Directories to scan for JSON files",
    )
    parser.add_argument(
        "--init-db",
        action="store_true",
        help="Create database tables before importing",
    )
    args = parser.parse_args(argv)

    if args.init_db:
        init_db()
    import_paths(args.paths)


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()
